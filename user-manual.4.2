    		    IRST Language Model Toolkit
			     Version 4.2
   		             May 26, 2006
		     
			     USER MANUAL

----------------------------------------------------------------------
1. Introduction
----------------------------------------------------------------------

This  is a short  tour around  the basic  functionalities of  the Irst
language  modeling (LM)  toolkit. It  should  quickly put  you in  the
condition of:

- generating n-gram statistics from a text file
- estimating n-gram LMs using different smoothing criteria
- saving a  LM into a file for further use,  i.e. speech recognition,
  machine translation, etc.

Among  state-of-the-art  n-gram  smoothing  techniques,  the  Irst  LM
toolkit  features LM  adaptation  methods which  can  be effective  if
limited task-related data are available.

Introductory material can be found in any of the following books:

- "Spoken Dialogues with Computers" by Renato DeMori, chapter 7.

- "Speech  and Language Processing"  by Dan  Jurafsky and  Jim Martin,
   Chapter 6.

- "Foundations   of  Statistical   Natural  Language   Processing"  by
   C. Manning and H. SchÃ¼tze, "

- "Statistical Methods for Speech Recognition", by Frederick Jelinek

- "Spoken Language Processing" by Huang, Acero and Hon.

With respect  to the previous version 3.2,  the following improvements
have been added:

- Quantization of probabilities
- Efficient run-time data structure for LM querying 
- Dismissal of MT output format

----------------------------------------------------------------------
2. Getting started
----------------------------------------------------------------------

Let  us see  how  to create  and  test a  trigram  LM.  The  directory
"example" contains two English  text files, namely "train" and "test",
which   we    will   use   to   estimate   and    evaluate   our   LM,
respectively. Notice that both file are tokenized and contain one 
sentence per line and sentence boundary symbols. Given a text file
split into lines, sentence boundary symbols can be added by using the 
script "add-start-end.sh" to each line as follows:

> ../util/add-start-end.sh < text> text-se

We  can estimate and  evaluate a  3-gram (trigram)  LM by  running the
command:

> tlm4.2 -tr=train -n=3 -lm=wb -te=test

which produces the output:

> n=49983 LP=301659.3944 PP= 417.8990388 OOVRate=0.02350799272

that tells, respectively: the number of  words in the test set, the LM
log-probability, perplexity  and out-of-vocabulary rate  over the test
set.

If one needs  to train and test different language  models on the same
data, a  more efficient way to  proceed is to first  create an trigram
table from the training data. This can be done with the command:

> ngt4.2 -i=train -n=3 -o=train.www -b=yes

which reads an  input text file, creates an  n-gram table of specified
size, and saves it in binary format into a specified output file.

Now,  a  LM can  be  more quickly  estimated  and  evaluated with  the
command:

> tlm4.2 -tr=train.www -n=3 -lm=wb -te=test

IMPORTANT: tlm4.2  and ngt4.2 can  create n-gram tables either  from a
text file or from another n-gram table file. 

Once  estimated, a  LM can  be  saved into  a file  with two  possible
formats: the standard ARPA text format or a binary format supported by
the IRST speech recognizer. The two following lines show both options:

> tlm4.2 -tr=train.www -n=3 -lm=wb -te=test -oarpa=train.lm
> tlm4.2 -tr=train.www -n=3 -lm=wb -te=test -oasr=train.lm

REMARK:  the IRST speech  recognizer format  permits to  represent LMs
with vocabulary size of at  most 64K words. See the following sections
about how to limit the vocabulary size of a LM.

---------------------------------------------------------------------
3. More on ngt4.2
----------------------------------------------------------------------

Given a text corpus we can compute its dictionary and word frequencies
with the command:

> dict4.2 -i=train -o=train.dict -f=yes

For speech recognition applications, it  can be often the case that we
want to limit the LM dictionary  only to the top frequent, let us say,
10K words. We can obtain such a list by:

> sort -rn +1 train.dict | grep -v "\(<s>\|</s>\)" | cut -d" " -f1 | head -10000 > top10k
 
A new  n-gram table for the  limited dictionary can  be computed table
with ngt4.2 by specifying the sub-dictionary:

> ngt4.2 -i=train.www -sd=top10k -n=3 -o=train.10k.www -b=yes

The command  will replace  all words outside  top10K with  the special
out-of-vocabulary symbol _unk_.

An other useful feature of ngt4.2 is the merging of two n-gram tables,
i.e. "text-a" and file "text-b", by means of the option -aug:

> ngt4.2 -i=text-a -aug=text-b -n=3 -o=a-b.www -b=yes

----------------------------------------------------------------------
4. More on tlm4.2
----------------------------------------------------------------------

Language models have to  cope with out-of-vocabulary words, hence they
estimate  some probability  for the  _unk_  word class.   In order  to
compare perplexity of LMs with  different vocabulary size it is better
to define  a conventional dictionary  size, or dictionary  upper bound
size,  trough the  parameter  (-dub).  In  the  following example,  we
compare the perplexity of the full vocabulary LM with the 10K-word LM,
by assuming a one million word dictionary upper bound.

>tlm4.2 -tr=train.10k.www -n=3 -lm=wb -te=test -dub=1000000
>n=49983 LP=370649.4859 PP= 1661.557994 OOVRate=0.1310445551

>tlm4.2 -tr=train.www -n=3 -lm=wb -te=test -dub=1000000
>n=49983 LP=317840.7683 PP= 577.6553547 OOVRate=0.02350799272

The  large  difference  in  perplexity  is  mainly  explained  by  the
significantly higher OOV rate of the 10K-word LM.

N-gram LMs generally apply frequency smoothing techniques, and combine
smoothed frequencies according to  two main schemes: interpolation and
back-off.  For  historical reasons, this  toolkit assumes interpolation
as default.  The back-off  scheme is computationally more expensive but
often provide better performance. It  can be activated with the option
"-bo=yes", e.g.:

>tlm4.2 -tr=train.10k.www -n=3 -lm=wb -te=test -dub=1000000 -bo=yes
> n=49983 LP=364374.1369 PP= 1465.514017 OOVRate=0.1310445551

This toolkit implements several frequency smoothing methods, which are
specified  by  the  parameter  "-lm".  Two  methods  are  particularly
recommended:

a) Modified shift-beta or  kneser-ney smoothing (msb).  This smoothing
scheme gives top performance when training data is not very sparse:

>tlm4.2 -tr=train.www -n=3 -lm=msb -te=test -dub=1000000 -bo=yes
> n=49983 LP=300588.455 PP= 409.0403458 OOVRate=0.02350799272

b) Witten Bell smoothing  (-lm=wb). This is an excellent smoothing
   method which works well in every data condition:

> tlm4.2 -tr=train.www -n=3 -lm=wb -te=test -dub=1000000 -bo=yes
> n=49983 LP=310772.6037 PP= 501.4811638 OOVRate=0.02350799272

Using an  n-gram table  with a fixed  (limited) dictionary  will cause
some performance  degradation, as LM smoothing  statistics will result
slightly distorted. A  valid alternative is to estimate  the LM on the
full dictionary of the training corpus and to use a limited dictionary
just when  saving the  LM on a  file.  This  can be achieved  with the
option -d (-dictionary):

> tlm4.2 -tr=train.www -n=3 -lm=msb -bo=y -te=test -oarpa=train.lm -d=top10k
> tlm4.2 -tr=train.www -n=3 -lm=msb -bo=y -te=test -oasr=train.asr -d=top10k


----------------------------------------------------------------------
5. LM Adaptation
----------------------------------------------------------------------

Language model adaptation can be  applied when little training data is
given for the task at hand, but much more data from other less related
sources is available.  tlm4.2 supports two adaptation methods.

5.1. Minimum Discriminative Information Adaptation. 

MDI adaptation  is used  when domain related  data is very  little but
enough to  estimate a  unigram LM.  Basically,  the n-gram probs  of a
general  purpose (background)  LM are  scaled so  that they  match the
target unigram distribution.
	 
Relevant parameters:

-ar=<V>: the adaptation rate, where <V> is a number ranging 
 from 0 (=no adaptation) to 1(=strong adaptation).

-ad=<file>: the  adaptation file,  where <file> is  either a text  or a
  unigram table.

-ao=y: open vocabulary mode, which  must be set if the adaptation file
 contains new words.

As an example, we apply MDI adaptation on the "adapt" file:

> tlm4.2 -tr=train.www -lm=wb -n=3 -te=test -dub=1000000 -ad=adapt -ar=0.8 -ao=yes
> n=49983 LP=292567.2241 PP= 348.3942064 OOVRate=0.01070363924

IMPORTANT:  ModifiedShiftBeta  smoothing  cannot  be applied  in  open
vocabulary mode  (-ao=yes).  If  this is the  case, you  should either
change  smoothing method  or simply  add  the adaptation  text to  the
background LM (use -aug parameter  of ngt4.2). This solution should in
general provide better performance.

> ngt4.2 -i=train.www -aug=adapt -o=train-adapt.www -n=3
> tlm4.2 -tr=train-adapt.www -lm=msb -n=3 -te=test -dub=1000000 -ad=adapt -ar=0.8
> n=49983 LP=265465.3621 PP= 202.5755738 OOVRate=0.01070363924

5.2 Mixture Adaptation

Mixture adaptation  is useful  when you have  enough training  data to
estimate a  bigram or  trigram LM and  you also have  data collections
from other domains.

Relevant parameters:

-lm=mix : specifies mixture smoothing method
-slmi=<filename>: specifies filename with information about LMs to combine.

In the example directory, the file sublmi contains the following lines:
>3
>-slm=wb  -str=adapt -sp=0
>-slm=msb -str=b.www -sp=0
>-slm=msb -str=a.www -sp=0


This means  that we use train a  mixture model on the  "adapt" set and
combine it  with the "a" and "b"  sets. For each data  set the desired
smoothing method is specified  (disregard the parameter -sp). The file
used for adaptation is the one in last position.

> tlm4.2 -tr=a.www -lm=mix -slmi=sublm -n=3 -te=test -dub=1000000
> n=49983 LP=312950.7551 PP= 523.8177876 OOVRate=0.02350799272

IMPORTANT: for  computational reasons it  is expected that  the n-gram
table  specified by -tr  contains AT  LEAST the  n-grams of  the last
table specified in the slmi file, i.e. "a.www" in  the example.
Faster computations are achieved by putting the largest dataset as the
last sub-model in the list and the union of all data sets as training
file.

It is  also IMPORTANT  that a  large -dub value  is specified  so that
probabilities  of  sub-LMs  can  be  correctly  computed  in  case  of
out-of-vocabulary words.

----------------------------------------------------------------------
6. LM Output Formats 
----------------------------------------------------------------------

This toolkit supports two output format of LMs. These formats have the
purpose of permitting  the use of LMs by  external programs.  External
programs could in principle estimate the LM from an ngram table before
using it, but this takes much more time and memory! So it is better to
first  estimate the LM  and then  use a  binary representation  of it,
which can be uploaded quickly and requires much less memory.

6.1 ARPA Format

This format was  introduced in DARPA ASR evaluations  to exchange LMs.
ARPA format  is also  supported by the  SRI LM  Toolkit. It is  a text
format which is rather costly in terms of memory. There is no limit to
the size n of n-grams.

6.2 ASR Format

This  format is  used  to interface  this  toolkit with  the Irst  ASR
system.   It permits  at  maximum  65,536 words  and  only works  with
bigrams  and   trigrams  LMs.   From   this  file  a   static  network
representation is built which is then uploaded by the ASR system.


----------------------------------------------------------------------
7. LM Quantization and Language Model Table Format
----------------------------------------------------------------------

A language model file in ARPA  format can be quantized and stored in a
compact data structure, called language model table. 

Quantization can be performed by the command:

> qlm4.2 -i train.lm -o train.qlm

which  generates   a  quantized  version  of  the   LM  which  encoded
probabilities in 8 bits. The  output is a modified ARPA format, called
qARPA.

LMs in ARPA or qARPA format can be stored in a compact binary table
through the lmt command:

> lmt4.2 -i=train.qlm -o=train.bqlm -n=4 -b=y

The resulting binary LM can be queried through the command:

> lmt4.2 -i=train.qblm -n=4 -itlm=yes

which reads a text from input and returns word by word LM probabilities.

----------------------------------------------------------------------
Author:

Marcello Federico
ITC-irst, Trento, Italy

federico@itc.it
----------------------------------------------------------------------







 



